{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTvg4C_-6F5R"
   },
   "source": [
    "# Generation of sentences from vector representations of sentences\n",
    "\n",
    "In this notebook, we will see how to train a decoder to generate sentences from sentence vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bzS5-Hfw6DHc"
   },
   "outputs": [],
   "source": [
    "# !python3 -m venv venv_vec2seq\n",
    "# !source venv_vec2seq/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJoiTSPS6OQS"
   },
   "source": [
    "## Installation\n",
    "To be able to run vec2seq, you need to install packages including:\n",
    "* sentence-transformers\n",
    "* gensim\n",
    "* sacrebleu\n",
    "* distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DTbs_psq6Qh9"
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement with CUDA if your system supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G1tmpS0X6ViS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2mt4wjU6YQp"
   },
   "source": [
    "## Training a decoder for sentence vectors\n",
    "We generate sentences on the premise of having only a fixed-size vector and no other information.\n",
    "\n",
    "For that, we train a vector-to-sequence model which is a unidirectional RNN (decoder part of RNN Encoder-Decoder).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading sentence embedding model\n",
    "\n",
    "we implement four typical methods for representing variable-length sentences into fixed-size vectors:\n",
    "* *'word-vec-sum'*:  summation over word vectors\n",
    "* *'word-vec-avg'*: word vectors averaging\n",
    "* *'word-vec-concat'*: concatenation of word vectors\n",
    "* *'bert-base-nli-mean-tokens'*: pre-trained sentence embedding model (SBERT)\n",
    "\n",
    "Note that you need to specify the path of the word2vec model if you choose the embedding methods based on word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ULcefkqv6eUq"
   },
   "outputs": [],
   "source": [
    "from vec2seq.sentence2vector import encoder\n",
    "\n",
    "encoder_name='word-vec-sum' \n",
    "encoder_path='/mango/homes/WANG_Liyan/model/cc.en.300.init.bin'\n",
    "\n",
    "encoder = encoder(model_name=encoder_name,model_path=encoder_path, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n943W3xs6iyo"
   },
   "source": [
    "print out the dimension of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zxWZ2wjH6jlx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "print('vector dimension: {}'.format(encoder.vector_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9Zx4jz26lkX"
   },
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Place data files in a specific directory, and pass the path to data_dir. (one sentence per line in each .tsv file.)\n",
    "\n",
    "We experiment with a toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F1xTqUL96nvi"
   },
   "outputs": [],
   "source": [
    "data_dir='/mango/homes/WANG_Liyan/data/vec2seq_toy/'\n",
    "train_filename='sent.train.tsv'\n",
    "valid_filename='sent.valid.tsv'\n",
    "test_filename='sent.test.tsv'\n",
    "\n",
    "batch_size=128\n",
    "max_length=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customize special tokens:\n",
    "* init_token: start of sentence (default as 'SOS')\n",
    "* eos_token: end of sentence (default as 'EOS')\n",
    "* pad_token: padding token (default as 'PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_token='SOS'\n",
    "eos_token = 'EOS'\n",
    "pad_token='PAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use `TextDataLoader` to do batching, padding, and building a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "za_S0QQN6qhP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 139\n"
     ]
    }
   ],
   "source": [
    "from vec2seq.dataloader import TextDataLoader\n",
    "\n",
    "dataset = TextDataLoader(data_dir=data_dir,\n",
    "                         train_filename=train_filename,\n",
    "                         valid_filename=valid_filename,\n",
    "                         test_filename=test_filename,\n",
    "                         batch_size=batch_size,\n",
    "                         max_length=max_length,\n",
    "                         device=device)\n",
    "\n",
    "vocab = dataset.generate_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZyXAyUx16sIf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/valid/test size: 60/20/6\n"
     ]
    }
   ],
   "source": [
    "train_set,valid_set,test_set = dataset.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPhgZKKA6vAs"
   },
   "source": [
    "### Customizing a decoder\n",
    "\n",
    "* the choice of model type: `decoder_net='basicrnn'` or `decoder_net='conrnn'`\n",
    "* the choice of loss function: `loss = 'sl'` or `loss = 'ml'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vec2seq.networks import basicRNN,ConRNN\n",
    "from vec2seq.networks import init_weights,count_parameters\n",
    "\n",
    "decoder_net='basicrnn'\n",
    "vocab_size=len(vocab.vocab)\n",
    "vector_dim = encoder.vector_size\n",
    "hid_dim = encoder.vector_size\n",
    "n_layers = 1\n",
    "loss = 'sl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KqnBt_Ux6w-D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Wang_Liyan/venv_vec2seq/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basicRNN(\n",
      "  (rnn): LSTM(300, 300, dropout=0.4)\n",
      "  (fc_out): Linear(in_features=300, out_features=139, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "The model has 764239 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "if decoder_net == \"conrnn\":\n",
    "    decoder = ConRNN(vocab_size=vocab_size , vector_dim=vector_dim, hid_dim=hid_dim, n_layers=n_layers, loss=loss)\n",
    "elif decoder_net == \"basicrnn\":\n",
    "    decoder = basicRNN(vocab_size=vocab_size , vector_dim=vector_dim, hid_dim=hid_dim, n_layers=n_layers, loss=loss)\n",
    "\n",
    "decoder = decoder.to(device)\n",
    "print(decoder)\n",
    "print('The model has {} trainable parameters'.format(count_parameters(decoder)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8E_y6kk161O0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basicRNN(\n",
       "  (rnn): LSTM(300, 300, dropout=0.4)\n",
       "  (fc_out): Linear(in_features=300, out_features=139, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We pass model arguments to the `trainer` class, and train the model by calling the function `train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TGWXaXAU62s6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Wang_Liyan/Pycharm_projects/vec2seq_git/vec2seq/sentence2vector.py:47: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.FloatTensor(vec).view(1,1,-1).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Time: 0m 0s\n",
      "\tTrain Loss: 4.922 | Train PPL: 137.338\n",
      "\t Val. Loss: 4.890 |  Val. PPL: 132.974\n",
      "Validation loss decreased (inf --> 4.8901567459106445).  Saving model ...\n",
      "Epoch: 1 | Time: 0m 0s\n",
      "\tTrain Loss: 4.874 | Train PPL: 130.878\n",
      "\t Val. Loss: 4.842 |  Val. PPL: 126.706\n",
      "Validation loss decreased (4.8901567459106445 --> 4.8418684005737305).  Saving model ...\n",
      "Epoch: 2 | Time: 0m 0s\n",
      "\tTrain Loss: 4.821 | Train PPL: 124.116\n",
      "\t Val. Loss: 4.790 |  Val. PPL: 120.309\n",
      "Validation loss decreased (4.8418684005737305 --> 4.790066719055176).  Saving model ...\n",
      "Epoch: 3 | Time: 0m 0s\n",
      "\tTrain Loss: 4.764 | Train PPL: 117.268\n",
      "\t Val. Loss: 4.734 |  Val. PPL: 113.702\n",
      "Validation loss decreased (4.790066719055176 --> 4.733580112457275).  Saving model ...\n",
      "Epoch: 4 | Time: 0m 0s\n",
      "\tTrain Loss: 4.699 | Train PPL: 109.851\n",
      "\t Val. Loss: 4.666 |  Val. PPL: 106.307\n",
      "Validation loss decreased (4.733580112457275 --> 4.666333198547363).  Saving model ...\n",
      "Epoch: 5 | Time: 0m 0s\n",
      "\tTrain Loss: 4.618 | Train PPL: 101.244\n",
      "\t Val. Loss: 4.578 |  Val. PPL:  97.306\n",
      "Validation loss decreased (4.666333198547363 --> 4.577855587005615).  Saving model ...\n",
      "Epoch: 6 | Time: 0m 0s\n",
      "\tTrain Loss: 4.513 | Train PPL:  91.200\n",
      "\t Val. Loss: 4.455 |  Val. PPL:  86.095\n",
      "Validation loss decreased (4.577855587005615 --> 4.455445766448975).  Saving model ...\n",
      "Epoch: 7 | Time: 0m 0s\n",
      "\tTrain Loss: 4.373 | Train PPL:  79.247\n",
      "\t Val. Loss: 4.287 |  Val. PPL:  72.745\n",
      "Validation loss decreased (4.455445766448975 --> 4.286954402923584).  Saving model ...\n",
      "Epoch: 8 | Time: 0m 0s\n",
      "\tTrain Loss: 4.185 | Train PPL:  65.674\n",
      "\t Val. Loss: 4.086 |  Val. PPL:  59.505\n",
      "Validation loss decreased (4.286954402923584 --> 4.086067199707031).  Saving model ...\n",
      "Epoch: 9 | Time: 0m 0s\n",
      "\tTrain Loss: 3.963 | Train PPL:  52.632\n",
      "\t Val. Loss: 3.909 |  Val. PPL:  49.873\n",
      "Validation loss decreased (4.086067199707031 --> 3.9094865322113037).  Saving model ...\n",
      "saved model.\n"
     ]
    }
   ],
   "source": [
    "from vec2seq.train import trainer\n",
    "\n",
    "epochs=10\n",
    "patience=5\n",
    "model_save_path='results/vec2seq_toy.pt'\n",
    "\n",
    "\n",
    "trainer = trainer(decoder=decoder,encoder=encoder,train_set=train_set,valid_set=valid_set,valid_path=data_dir+valid_filename,batch_size=batch_size,max_length=max_length,\n",
    "    corpora_field=vocab,loss_mode=loss,epochs=epochs,patience=patience,model_save_path=model_save_path,device=device)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO6CP0eY65iG"
   },
   "source": [
    "## Generating sentences from vectors using a trained decoder\n",
    "\n",
    "Once the training is completed, we can use the model to generate sentence from vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S7GopDuq67eo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_decoder_path = model_save_path\n",
    "decoder.load_state_dict(torch.load(pretrained_decoder_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `test` function to compute test loss and generate a list of reference and predicted sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wQg4o5Kk682r"
   },
   "outputs": [],
   "source": [
    "from vec2seq.test import test\n",
    "\n",
    "sentence_save_path='results/sents_toy.tsv'\n",
    "test_path=data_dir+test_filename\n",
    "\n",
    "results,test_loss = test(encoder=encoder,decoder=decoder,test_set=test_set,\n",
    "                         test_path=test_path,results_save_path=sentence_save_path,\n",
    "                         batch_size=batch_size,corpora_field=vocab,loss_mode=loss,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference                       Generation\n",
      "------------------------------  ----------------------\n",
      "I do not want to do it again .  I do not not not not .\n",
      "I do not want to go with you .  I do not not not .\n",
      "I do not want to hurt anyone .  I do not not not .\n",
      "Tom does not want to die .      I do not not not .\n",
      "I do not want to die .          I do not not not .\n",
      "It 's him                       I do not not .\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(results, headers=[\"Reference\",\"Generation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then evaluate sentences generated by the decoder trained on toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy data size: train 60 | valid 20 | test 6\n"
     ]
    }
   ],
   "source": [
    "print(\"toy data size: train {} | valid {} | test {}\".format(len(train_set.dataset),len(valid_set.dataset),len(test_set.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wSF6gjdx6-c-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------  ----\n",
      "BLEU               13\n",
      "Accuracy            0\n",
      "Distance in words   4.5\n",
      "Distance in chars  12\n",
      "-----------------  ----\n"
     ]
    }
   ],
   "source": [
    "from vec2seq.eval import eval\n",
    "scores = eval(results)\n",
    "\n",
    "\n",
    "print(tabulate(scores.items()))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vector_decoding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
